# OmniMRI

- [Introduction](#introduction)
- [Getting Started](#getting-started)
- [Publication](#publication)
- [Contacts](#contacts)
<!-- - [Star History](#star-history)-->

## Introduction
**OmniMRI** is a visionâ€“language foundation model that unifies the full MRI workflow, including reconstruction, segmentation, detection, diagnosis, and report generation, within a single multimodal architecture. Trained on 60 public datasets with 220,000+ MRI volumes and 19M slices, OmniMRI integrates imaging and clinical language through a multi-stage training paradigm, enabling inference across anatomies, contrasts, and tasks.

ðŸ“„ [Read the paper on arXiv](https://arxiv.org/abs/2508.17524v1)



## Getting Started
Coming soon...

### Publication
```bibtex
@misc{he2025omnimriunifiedvisionlanguagefoundation,
      title={OmniMRI: A Unified Vision--Language Foundation Model for Generalist MRI Interpretation}, 
      author={Xingxin He and Aurora Rofena and Ruimin Feng and Haozhe Liao and Zhaoye Zhou and Albert Jang and Fang Liu},
      year={2025},
      eprint={2508.17524},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2508.17524}, 
}
```

### Contacts
[Intelligent Imaging Innovation and Translation Lab](https://liulab.mgh.harvard.edu/) [[github]](https://github.com/I3Tlab) at the Athinoula A. Martinos Center of Massachusetts General Hospital and Harvard Medical School
* Xingxin He (xihe2@mgh.harvard.edu)
* Fang Liu (fliu12@mgh.harvard.edu)** â€“ Corresponding Author  

149 13th Street, Suite 2301
Charlestown, Massachusetts 02129, USA

For specific code requests, please contact the corresponding author **Fang Liu**.
